{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torah scraping\n",
    "\n",
    "### Looking for hidden patterns.\n",
    "### and not so hidden patterns.\n",
    "### 2020-4-21\n",
    "### Joe Hostyk and Alex Zaloum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMATRIA = {\"א\": 1, \"ב\": 2, \"ג\": 3, \"ד\": 4, \"ה\": 5, \"ו\": 6, \"ז\": 7, \"ח\": 8, \"ט\": 9, \"י\": 10, \"כ\": 20, \"ך\": 20, \"ל\": 30, \"מ\": 40, \"ם\": 40, \"נ\": 50, \"ן\": 50, \"ס\": 60, \"ע\": 70, \"פ\": 80, \"ף\": 80, \"צ\": 90, \"ץ\": 90, \"ק\": 100, \"ר\": 200, \"ש\": 300, \"ת\": 400}\n",
    "ALEPH_BEIS = GMATRIA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: sofit letters\n",
    "## Define the otiyot categories\n",
    "## Different breakdowns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames(folder):\n",
    "    \"\"\"\n",
    "    Get all our file names for processing later.\n",
    "    Currently, doesn't recursively search.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Full path to the folder.\n",
    "\n",
    "    Returns:\n",
    "        filenames (list of strings)\n",
    "    \"\"\"\n",
    "\n",
    "    print (\"Getting filenames...\")\n",
    "\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if \".txt\" in file:\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "\n",
    "def makeWordDictionaryFromSefer(filename):\n",
    "    \n",
    "    seferWords = Counter()\n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    seferWords[word] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return seferWords\n",
    "\n",
    "\n",
    "def makeLetterDictionaryFromSefer(filename, allLetters):\n",
    "    \n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    for letter in word:\n",
    "                        allLetters[letter] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return allLetters  \n",
    "\n",
    "def makeNgramsDictionaryFromSefer(filename, allNgrams, sizeOfngram):\n",
    "    \n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    for letterIndex in range(len(word) - sizeOfngram + 1):\n",
    "    \n",
    "                        ngram = word[letterIndex:letterIndex + sizeOfngram]\n",
    "                        allNgrams[ngram] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return allNgrams  \n",
    "\n",
    "def getAllFiles(filenames):\n",
    "    \n",
    "    allWords = {}\n",
    "    allLetters = Counter()\n",
    "    allNgrams = Counter()\n",
    "    \n",
    "    for filename in filenames:\n",
    "        \n",
    "        seferName = filename.replace(\".txt\", \"\").split(\"/\")[-1]\n",
    "        print(seferName)\n",
    "#         allWords[seferName] = makeDictionaryFromSefer(filename)\n",
    "#         allLetters = makeLetterDictionaryFromSefer(filename, allLetters)\n",
    "        allNgrams = makeNgramsDictionaryFromSefer(filename, allNgrams, sizeOfngram = 2)\n",
    "\n",
    "#         raise\n",
    "    return allNgrams\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./texts\"\n",
    "filenames = getFileNames(folder)\n",
    "# allWords = getAllFiles(filenames)\n",
    "# allLetters = getAllFiles(filenames)\n",
    "allNgrams = getAllFiles(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which pairs never show up in the Torah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPossiblePairs = list(itertools.product(ALEPH_BEIS, ALEPH_BEIS))\n",
    "allPossiblePairs = [\"\".join(pair) for pair in allPossiblePairs]\n",
    "# allPossiblePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['בף', 'גט', 'גכ', 'גס', 'גצ', 'גץ', 'גק', 'דז', 'דט', 'דס', 'דצ', 'דץ', 'הף', 'הץ', 'זט', 'זס', 'זף', 'זצ', 'זץ', 'זש', 'חא', 'חע', 'טג', 'טז', 'טכ', 'טס', 'טצ', 'טץ', 'טק', 'כץ', 'ךא', 'ךב', 'ךג', 'ךד', 'ךה', 'ךו', 'ךז', 'ךח', 'ךט', 'ךי', 'ךכ', 'ךך', 'ךל', 'ךמ', 'ךם', 'ךנ', 'ךן', 'ךס', 'ךע', 'ךפ', 'ךף', 'ךצ', 'ךץ', 'ךק', 'ךר', 'ךש', 'ךת', 'מף', 'םא', 'םב', 'םג', 'םד', 'םה', 'םו', 'םז', 'םח', 'םט', 'םי', 'םכ', 'םך', 'םל', 'םמ', 'םם', 'םנ', 'םן', 'םס', 'םע', 'םפ', 'םף', 'םצ', 'םץ', 'םק', 'םר', 'םש', 'םת', 'ןא', 'ןב', 'ןג', 'ןד', 'ןה', 'ןו', 'ןז', 'ןח', 'ןט', 'ןי', 'ןכ', 'ןך', 'ןל', 'ןמ', 'ןם', 'ןנ', 'ןן', 'ןס', 'ןע', 'ןפ', 'ןף', 'ןצ', 'ןץ', 'ןק', 'ןר', 'ןש', 'ןת', 'סז', 'סט', 'סצ', 'סץ', 'סש', 'עח', 'עע', 'עף', 'פב', 'פפ', 'ףא', 'ףב', 'ףג', 'ףד', 'ףה', 'ףו', 'ףז', 'ףח', 'ףט', 'ףי', 'ףכ', 'ףך', 'ףל', 'ףמ', 'ףם', 'ףנ', 'ףן', 'ףס', 'ףע', 'ףפ', 'ףף', 'ףצ', 'ףץ', 'ףק', 'ףר', 'ףש', 'ףת', 'צז', 'צס', 'צש', 'ץא', 'ץב', 'ץג', 'ץד', 'ץה', 'ץו', 'ץז', 'ץח', 'ץט', 'ץי', 'ץכ', 'ץך', 'ץל', 'ץמ', 'ץם', 'ץנ', 'ץן', 'ץס', 'ץע', 'ץפ', 'ץף', 'ץצ', 'ץץ', 'ץק', 'ץר', 'ץש', 'ץת', 'קג', 'קז', 'שצ', 'שץ']\n"
     ]
    }
   ],
   "source": [
    "print([pair for pair in allPossiblePairs if pair not in allNgrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common pairs in the Torah?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('את', 5543), ('וי', 4352), ('אל', 4090), ('ים', 3970), ('יה', 3734)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allNgrams.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('טד', 5),\n",
       " ('סס', 5),\n",
       " ('קא', 4),\n",
       " ('כט', 4),\n",
       " ('קף', 4),\n",
       " ('זם', 4),\n",
       " ('חף', 3),\n",
       " ('א\\u200d', 3),\n",
       " ('\\u200dש', 3),\n",
       " ('זך', 3),\n",
       " ('שף', 3),\n",
       " ('עס', 3),\n",
       " ('זח', 3),\n",
       " ('קק', 3),\n",
       " ('אט', 3),\n",
       " ('פמ', 3),\n",
       " ('נץ', 2),\n",
       " ('תץ', 2),\n",
       " ('לץ', 2),\n",
       " ('פף', 2),\n",
       " ('זפ', 2),\n",
       " ('צץ', 2),\n",
       " ('טט', 1),\n",
       " ('אץ', 1),\n",
       " ('סן', 1),\n",
       " ('אא', 1),\n",
       " ('צט', 1),\n",
       " ('קכ', 1),\n",
       " ('הך', 1),\n",
       " ('זג', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Most common:\n",
    "allNgrams.most_common(575)\n",
    "\n",
    "### Least common:\n",
    "allNgrams.most_common(575)[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.]),\n",
       " array([    3,    70,    70,   830,  1035,  1806,  1836,  2111,  2199,\n",
       "         2937,  3358,  3976,  4260,  4700,  7039,  7194,  8614,  9889,\n",
       "        10630, 11270, 14474, 15605, 16357, 17965, 18147, 21583, 27069,\n",
       "        28085, 30596, 31607]),\n",
       " <a list of 29 Patch objects>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sortedLetters = {k: v for k, v in sorted(allLetters.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.hist(list(sortedLetters.keys()), list(sortedLetters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Todo: make a heatmap of 24x24 letters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get gmatrias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmatrifyAword(word):\n",
    "    \"\"\"\n",
    "    Get the gmatria for one word. Doesn't catch punctuation/errors.\n",
    "\n",
    "    Args:\n",
    "        word (str)\n",
    "\n",
    "    Returns:\n",
    "        The gmatria (int)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return sum([GMATRIA[letter] for letter in word])\n",
    "\n",
    "# Numbers, to words with that gmatria\n",
    "\n",
    "def getGmatrias():\n",
    "    \n",
    "    gmatriasToWords = {}\n",
    "\n",
    "    directory = \"./texts/\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(\"./texts\")\n",
    "    \n",
    "    pathToGmatriaFile = \"{}bamidbarGmatriaByNumber.tsv\".format(directory)\n",
    "\n",
    "    if os.path.exists(pathToGmatriaFile):\n",
    "        print (\"Gmatria file exists. Reading...\")\n",
    "        reader = csv.reader(open(pathToGmatriaFile), delimiter = \"\\t\")\n",
    "        header = next(reader)\n",
    "        for line in reader:\n",
    "            line = dict(zip(header, line))\n",
    "            word = int(line[\"Gmatria\"])\n",
    "            shifts = set(line[\"Word\"].split(\" | \"))\n",
    "            gmatriasToWords[word] = shifts\n",
    "        print (\"Finished reading.\")\n",
    "    ### To-do: fill this in later\n",
    "#     else:\n",
    "#         print (\"Gmatria file does not exist. Creating...\")\n",
    "#         with open(pathToGmatriaFile, \"w\") as out:\n",
    "\n",
    "#             out.write(\"Gmatria\\tWord\\n\")\n",
    "\n",
    "#             \n",
    "    return gmatriasToWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gmatria file exists. Reading...\n",
      "Finished reading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'הארדי', 'וטהר', 'וידר', 'וירד', 'ורוח', 'טהור', 'יבחר', 'צפים', 'רוחו'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmatriasToWords = getGmatrias()\n",
    "gmatriasToWords[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames...\n",
      "vayikra\n",
      "dvarim\n",
      "breishit\n",
      "shmot\n",
      "bamidbar\n"
     ]
    }
   ],
   "source": [
    "folder = \"texts/Torah\"\n",
    "filenames = getFileNames(folder)\n",
    "\n",
    "sfarimToWords = {}\n",
    "for filename in filenames:\n",
    "    \n",
    "    seferName = filename.replace(\".txt\", \"\").split(\"/\")[-1]\n",
    "    print(seferName)\n",
    "    wordsInSefer = makeWordDictionaryFromSefer(filename)\n",
    "    sfarimToWords[seferName] = wordsInSefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bamidbar has 3847 words.\n",
      "bamidbar and dvarim share 1201 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and shmot share 1412 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and vayikra share 973 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and breishit share 1323 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and shmot share 1302 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and vayikra share 858 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and breishit share 1313 words.\n",
      "shmot has 4171 words.\n",
      "shmot and vayikra share 1007 words.\n",
      "shmot has 4171 words.\n",
      "shmot and breishit share 1482 words.\n",
      "vayikra has 2710 words.\n",
      "vayikra and breishit share 860 words.\n"
     ]
    }
   ],
   "source": [
    "sfarim = set(sfarimToWords)\n",
    "combos = itertools.combinations(sfarim, 2)\n",
    "for sefer1, sefer2 in combos:\n",
    "\n",
    "    sefer1words = set(sfarimToWords[sefer1])\n",
    "    sefer2words = set(sfarimToWords[sefer2])\n",
    "\n",
    "#     print (\"{} has {} words.\".format(sefer1, len(sefer1words)))\n",
    "#     print (\"{} has {} words.\".format(sefer2, len(sefer2words)))\n",
    "    print (\"{} and {} share {} words.\".format(sefer1, sefer2, len(sefer1words.intersection(sefer2words))))\n",
    "#     print (\"{} has {} unique words.\".format(sefer1, len(sefer1words.difference(sefer2words))))\n",
    "#     print (\"{} has {} unique words.\".format(sefer2, len(sefer2words.difference(sefer1words))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic histogram/breakdowns:\n",
    "    \n",
    "#### Letters, pairs of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talmud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames...\n",
      "There are 37 masechtot.\n"
     ]
    }
   ],
   "source": [
    "## Load:\n",
    "folder = \"texts/Talmud\"\n",
    "masechetFilenames = getFileNames(folder)\n",
    "print (\"There are {} masechtot.\".format(len(masechetFilenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yikes\n",
    "def masechetFromFilename(masechetFilename):\n",
    "    \n",
    "    splitName = masechetFilename.split(\"/\")\n",
    "    nextIndex = splitName.index(\"English\")\n",
    "    masechetName = splitName[nextIndex - 1]\n",
    "    \n",
    "    return masechetName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran the spaCy code below to get all people in the Talmud. That had lots of false positives (e.g. \"Leviticus 8:11\", \"chews ginger\"), so took all lines with \"Rav\", \"Rabb\", and \"The\" in them, to get most of the true hits.\n",
    "(Missing people like \"Berurya\" or \"Ḥoni HaMe’aggel\" with this, though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 1666 rabbis from the Talmud.\n"
     ]
    }
   ],
   "source": [
    "rabbis = set([person.strip() for person in open(\"RavRabbThe.txt\", \"r\")])\n",
    "print(\"Working with {} rabbis from the Talmud.\".format(len(rabbis)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niddah\n",
      "Chagigah\n",
      "Yoma\n",
      "Rosh Hashanah\n",
      "Moed Katan\n",
      "Beitzah\n",
      "Sukkah\n",
      "Megillah\n",
      "Taanit\n",
      "Pesachim\n",
      "Shabbat\n",
      "Eruvin\n",
      "Berakhot\n",
      "Horayot\n",
      "Avodah Zarah\n",
      "Bava Batra\n",
      "Shevuot\n",
      "Sanhedrin\n",
      "Bava Metzia\n",
      "Bava Kamma\n",
      "Makkot\n",
      "Sotah\n",
      "Kiddushin\n",
      "Nazir\n",
      "Yevamot\n",
      "Gittin\n",
      "Ketubot\n",
      "Nedarim\n",
      "Zevachim\n",
      "Meilah\n",
      "Bekhorot\n",
      "Temurah\n",
      "Keritot\n",
      "Menachot\n",
      "Arakhin\n",
      "Chullin\n",
      "Tamid\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lineNumber = 0\n",
    "people = set()\n",
    "rabbisToMasechet = defaultdict(Counter)\n",
    "masechetToRabbis = defaultdict(Counter)\n",
    "\n",
    "for filename in masechetFilenames:\n",
    "    \n",
    "    masechetName = masechetFromFilename(filename)\n",
    "    print (\"\\r{}\\r\".format(masechetName))\n",
    "    reader = open(filename, \"r\")\n",
    "    for line in reader:\n",
    "        \n",
    "        ### Skip the header info, and the whitespace/daf numbers\n",
    "        lineNumber += 1\n",
    "        if lineNumber < 21 or len(line) < 13:\n",
    "            continue\n",
    "        line = re.sub('<[^<]+?>', '', line) # ayy https://stackoverflow.com/a/4869782\n",
    "\n",
    "        ### Incredible https://spacy.io/usage/linguistic-features#named-entities\n",
    "        doc = nlp(line)            \n",
    "        for entity in doc.ents:\n",
    "            \n",
    "            ### First pass: get all the people (lots of false positives):\n",
    "#             if entity.label_ == \"PERSON\" and entity.text in rabbis:\n",
    "#                 people.add(entity)\n",
    "                \n",
    "            ### Next round: use that to make an edited list:\n",
    "            if entity.label_ == \"PERSON\" and entity.text in rabbis:\n",
    "                rabbisToMasechet[entity.text][masechetName] += 1\n",
    "                masechetToRabbis[masechetName][entity.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First round: save all the spaCy results.\n",
    "\n",
    "# peopleToText = {name.text for name in people}\n",
    "# with open(\"talmudPeople.txt\", \"w\") as out:\n",
    "#     out.write(\"\\n\".join(peopleToText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second round: save the more accurate counts.\n",
    "\n",
    "sortedRabbis = sorted(rabbis)\n",
    "with open(\"RabbiCounts.tsv\", \"w\") as out:\n",
    "    \n",
    "    masechtot = sorted(masechetToRabbis)\n",
    "    out.write(\"\\t{}\\n\".format(\"\\t\".join(masechtot)))\n",
    "    \n",
    "    for rabbi in sortedRabbis:\n",
    "        \n",
    "        rabbiCounts = rabbisToMasechet[rabbi]\n",
    "        sortedCounts = [str(rabbiCounts[masechet]) for masechet in masechtot]\n",
    "        \n",
    "        out.write(\"{r}\\t{c}\\n\".format(r = rabbi, c = \"\\t\".join(sortedCounts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Matching (without spaCy)\n",
    "\n",
    "Alternative (quicker) way that doesn't use spaCy, so it doesn't do true entity recognition and just searches for any matches. Runs instantly, but counts \"Rav\" a bunch because it counts \"Rav x\" for Rav as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rabbisToMasechet = defaultdict(Counter)\n",
    "# masechetToRabbis = defaultdict(Counter)\n",
    "\n",
    "# for filename in masechetFilenames:\n",
    "        \n",
    "#     masechetName = masechetFromFilename(filename)\n",
    "#     print (\"\\r{}\\r\".format(masechetName))\n",
    "#     with open(filename, \"r\") as masechet:\n",
    "        \n",
    "#         text = masechet.read()\n",
    "#         for rabbi in rabbis:\n",
    "#             appearances = text.count(rabbi)\n",
    "            \n",
    "#             if appearances != 0:\n",
    "#                 rabbisToMasechet[rabbi][masechetName] += appearances\n",
    "#                 masechetToRabbis[masechetName][rabbi] += appearances\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Big to print\n",
    "# print(rabbisToMasechet)\n",
    "# print(masechetToRabbis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbi Yehuda's most common:\n",
      " \tShabbat: 446; Pesachim: 409; Chullin: 406; Eruvin: 395; Menachot: 322\n",
      "Berakhot's most common:\n",
      " \tRabbi Yoḥanan: 292; Rabbi Yehuda: 222; Rav: 219; Rava: 122; Rabbi Yosei: 110\n",
      "\n",
      "Rabbi Yehuda's least common:\n",
      " \tHorayot: 48; Rosh Hashanah: 40; Chagigah: 37; Meilah: 16; Tamid: 2\n",
      "Berakhot's least common:\n",
      " \tRav Sama: 1; Rabbi Yosei bar Yehuda: 1; Rabbi Parnakh: 1; Rav Huna bar Berekhya: 1; Rabbi Elazar HaKappar: 1; Rabbi Yosei ben Keifar: 1; Zekharya ben Kevutal: 1; Zekharya ben: 1; the Sages of the Mishna: 1; Rav Yehuda bar Zevida: 1\n"
     ]
    }
   ],
   "source": [
    "def formattedCounts(counts):\n",
    "    \n",
    "    formatted = \"\\t\" + \"; \".join([\"{}: {}\".format(value, count) for value, count in counts])\n",
    "    return formatted\n",
    "    \n",
    "rabbi = \"Rabbi Yehuda\"\n",
    "masechet = \"Berakhot\"### Most common:\n",
    "\n",
    "print(\"{}'s most common:\\n\".format(rabbi), formattedCounts(rabbisToMasechet[rabbi].most_common(5)))\n",
    "print(\"{}'s most common:\\n\".format(masechet), formattedCounts(masechetToRabbis[masechet].most_common(5)))\n",
    "print()\n",
    "\n",
    "### Least common:\n",
    "\n",
    "print(\"{}'s least common:\\n\".format(rabbi), formattedCounts(rabbisToMasechet[rabbi].most_common()[-5:]))\n",
    "print(\"{}'s least common:\\n\".format(masechet), formattedCounts(masechetToRabbis[masechet].most_common()[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rabbisFullCounts = Counter()\n",
    "\n",
    "for rabbi, masechetCounts in rabbisToMasechet.items():\n",
    "    \n",
    "    masechetTotalCounts = sum(masechetCounts.values())\n",
    "    rabbisFullCounts[rabbi] += masechetTotalCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rabbi Yoḥanan', 6473),\n",
       " ('Rabbi Yehuda', 6335),\n",
       " ('Rav', 5819),\n",
       " ('Rabbi Shimon', 3915),\n",
       " ('Rabbi Meir', 3756),\n",
       " ('Rabbi Yosei', 3589),\n",
       " ('Rabbis', 3552),\n",
       " ('Rava', 3507),\n",
       " ('Rabbi Eliezer', 3123),\n",
       " ('Rabbi Yehuda HaNasi', 3079),\n",
       " ('Rabbi Akiva', 2926),\n",
       " ('Rabbi Elazar', 2712),\n",
       " ('Rav Huna', 2481),\n",
       " ('Rav Yehuda', 2432),\n",
       " ('Rav Ashi', 2241),\n",
       " ('Rav Pappa', 2027),\n",
       " ('Rav Yosef', 1965),\n",
       " ('Rav Ḥisda', 1742),\n",
       " ('Rabba', 1664),\n",
       " ('Rabbi Yishmael', 1583),\n",
       " ('Rabbi Ḥiyya', 1421),\n",
       " ('Rabbi Yehoshua', 1344),\n",
       " ('Rabbi Ḥanina', 1171),\n",
       " ('Rabbi Zeira', 1148),\n",
       " ('Rav Sheshet', 1029),\n",
       " ('Rabban Gamliel', 848),\n",
       " ('Rav Kahana', 764),\n",
       " ('Shimon ben Gamliel', 717),\n",
       " ('Ravina', 699),\n",
       " ('Rabbi Yehoshua ben Levi', 685)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rabbisFullCounts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (general)",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
