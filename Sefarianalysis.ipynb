{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torah scraping\n",
    "\n",
    "### Looking for hidden patterns.\n",
    "### and not so hidden patterns.\n",
    "### 2020-4-21\n",
    "### Joe Hostyk and Alex Zaloum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMATRIA = {\"א\": 1, \"ב\": 2, \"ג\": 3, \"ד\": 4, \"ה\": 5, \"ו\": 6, \"ז\": 7, \"ח\": 8, \"ט\": 9, \"י\": 10, \"כ\": 20, \"ך\": 20, \"ל\": 30, \"מ\": 40, \"ם\": 40, \"נ\": 50, \"ן\": 50, \"ס\": 60, \"ע\": 70, \"פ\": 80, \"ף\": 80, \"צ\": 90, \"ץ\": 90, \"ק\": 100, \"ר\": 200, \"ש\": 300, \"ת\": 400}\n",
    "ALEPH_BEIS = GMATRIA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: sofit letters\n",
    "## Define the otiyot categories\n",
    "## Different breakdowns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames(folder):\n",
    "    \"\"\"\n",
    "    Get all our file names for processing later.\n",
    "    Currently, doesn't recursively search.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Full path to the folder.\n",
    "\n",
    "    Returns:\n",
    "        filenames (list of strings)\n",
    "    \"\"\"\n",
    "\n",
    "    print (\"Getting filenames...\")\n",
    "\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if \".txt\" in file:\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "\n",
    "def makeWordDictionaryFromSefer(filename):\n",
    "    \n",
    "    seferWords = Counter()\n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    seferWords[word] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return seferWords\n",
    "\n",
    "\n",
    "def makeLetterDictionaryFromSefer(filename, allLetters):\n",
    "    \n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    for letter in word:\n",
    "                        allLetters[letter] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return allLetters  \n",
    "\n",
    "def makeNgramsDictionaryFromSefer(filename, allNgrams, sizeOfngram):\n",
    "    \n",
    "    with open(filename, \"r\") as psukim:\n",
    "        for pasuk in psukim:\n",
    "\n",
    "            cleanedPasuk = pasuk.strip().replace(\"־\", \" \").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            splitPasuk = cleanedPasuk.split(\" \")\n",
    "\n",
    "            # Skip the non-text lines.\n",
    "            if len(splitPasuk) == 1 or splitPasuk[0] == \"Chapter\":\n",
    "                continue\n",
    "\n",
    "            for word in splitPasuk:\n",
    "                try:\n",
    "                    for letterIndex in range(len(word) - sizeOfngram + 1):\n",
    "    \n",
    "                        ngram = word[letterIndex:letterIndex + sizeOfngram]\n",
    "                        allNgrams[ngram] += 1\n",
    "                except KeyError as e:\n",
    "                    problematicWords.add(word)\n",
    "    return allNgrams  \n",
    "\n",
    "def getAllFiles(filenames):\n",
    "    \n",
    "    allWords = {}\n",
    "    allLetters = Counter()\n",
    "    allNgrams = Counter()\n",
    "    \n",
    "    for filename in filenames:\n",
    "        \n",
    "        seferName = filename.replace(\".txt\", \"\").split(\"/\")[-1]\n",
    "        print(seferName)\n",
    "#         allWords[seferName] = makeDictionaryFromSefer(filename)\n",
    "#         allLetters = makeLetterDictionaryFromSefer(filename, allLetters)\n",
    "        allNgrams = makeNgramsDictionaryFromSefer(filename, allNgrams, sizeOfngram = 2)\n",
    "\n",
    "#         raise\n",
    "    return allNgrams\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./texts\"\n",
    "filenames = getFileNames(folder)\n",
    "# allWords = getAllFiles(filenames)\n",
    "# allLetters = getAllFiles(filenames)\n",
    "allNgrams = getAllFiles(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which pairs never show up in the Torah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPossiblePairs = list(itertools.product(ALEPH_BEIS, ALEPH_BEIS))\n",
    "allPossiblePairs = [\"\".join(pair) for pair in allPossiblePairs]\n",
    "# allPossiblePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['בף', 'גט', 'גכ', 'גס', 'גצ', 'גץ', 'גק', 'דז', 'דט', 'דס', 'דצ', 'דץ', 'הף', 'הץ', 'זט', 'זס', 'זף', 'זצ', 'זץ', 'זש', 'חא', 'חע', 'טג', 'טז', 'טכ', 'טס', 'טצ', 'טץ', 'טק', 'כץ', 'ךא', 'ךב', 'ךג', 'ךד', 'ךה', 'ךו', 'ךז', 'ךח', 'ךט', 'ךי', 'ךכ', 'ךך', 'ךל', 'ךמ', 'ךם', 'ךנ', 'ךן', 'ךס', 'ךע', 'ךפ', 'ךף', 'ךצ', 'ךץ', 'ךק', 'ךר', 'ךש', 'ךת', 'מף', 'םא', 'םב', 'םג', 'םד', 'םה', 'םו', 'םז', 'םח', 'םט', 'םי', 'םכ', 'םך', 'םל', 'םמ', 'םם', 'םנ', 'םן', 'םס', 'םע', 'םפ', 'םף', 'םצ', 'םץ', 'םק', 'םר', 'םש', 'םת', 'ןא', 'ןב', 'ןג', 'ןד', 'ןה', 'ןו', 'ןז', 'ןח', 'ןט', 'ןי', 'ןכ', 'ןך', 'ןל', 'ןמ', 'ןם', 'ןנ', 'ןן', 'ןס', 'ןע', 'ןפ', 'ןף', 'ןצ', 'ןץ', 'ןק', 'ןר', 'ןש', 'ןת', 'סז', 'סט', 'סצ', 'סץ', 'סש', 'עח', 'עע', 'עף', 'פב', 'פפ', 'ףא', 'ףב', 'ףג', 'ףד', 'ףה', 'ףו', 'ףז', 'ףח', 'ףט', 'ףי', 'ףכ', 'ףך', 'ףל', 'ףמ', 'ףם', 'ףנ', 'ףן', 'ףס', 'ףע', 'ףפ', 'ףף', 'ףצ', 'ףץ', 'ףק', 'ףר', 'ףש', 'ףת', 'צז', 'צס', 'צש', 'ץא', 'ץב', 'ץג', 'ץד', 'ץה', 'ץו', 'ץז', 'ץח', 'ץט', 'ץי', 'ץכ', 'ץך', 'ץל', 'ץמ', 'ץם', 'ץנ', 'ץן', 'ץס', 'ץע', 'ץפ', 'ץף', 'ץצ', 'ץץ', 'ץק', 'ץר', 'ץש', 'ץת', 'קג', 'קז', 'שצ', 'שץ']\n"
     ]
    }
   ],
   "source": [
    "print([pair for pair in allPossiblePairs if pair not in allNgrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common pairs in the Torah?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('את', 5543), ('וי', 4352), ('אל', 4090), ('ים', 3970), ('יה', 3734)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allNgrams.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('טד', 5),\n",
       " ('סס', 5),\n",
       " ('קא', 4),\n",
       " ('כט', 4),\n",
       " ('קף', 4),\n",
       " ('זם', 4),\n",
       " ('חף', 3),\n",
       " ('א\\u200d', 3),\n",
       " ('\\u200dש', 3),\n",
       " ('זך', 3),\n",
       " ('שף', 3),\n",
       " ('עס', 3),\n",
       " ('זח', 3),\n",
       " ('קק', 3),\n",
       " ('אט', 3),\n",
       " ('פמ', 3),\n",
       " ('נץ', 2),\n",
       " ('תץ', 2),\n",
       " ('לץ', 2),\n",
       " ('פף', 2),\n",
       " ('זפ', 2),\n",
       " ('צץ', 2),\n",
       " ('טט', 1),\n",
       " ('אץ', 1),\n",
       " ('סן', 1),\n",
       " ('אא', 1),\n",
       " ('צט', 1),\n",
       " ('קכ', 1),\n",
       " ('הך', 1),\n",
       " ('זג', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Most common:\n",
    "allNgrams.most_common(575)\n",
    "\n",
    "### Least common:\n",
    "allNgrams.most_common(575)[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.]),\n",
       " array([    3,    70,    70,   830,  1035,  1806,  1836,  2111,  2199,\n",
       "         2937,  3358,  3976,  4260,  4700,  7039,  7194,  8614,  9889,\n",
       "        10630, 11270, 14474, 15605, 16357, 17965, 18147, 21583, 27069,\n",
       "        28085, 30596, 31607]),\n",
       " <a list of 29 Patch objects>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sortedLetters = {k: v for k, v in sorted(allLetters.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.hist(list(sortedLetters.keys()), list(sortedLetters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Todo: make a heatmap of 24x24 letters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get gmatrias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmatrifyAword(word):\n",
    "    \"\"\"\n",
    "    Get the gmatria for one word. Doesn't catch punctuation/errors.\n",
    "\n",
    "    Args:\n",
    "        word (str)\n",
    "\n",
    "    Returns:\n",
    "        The gmatria (int)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return sum([GMATRIA[letter] for letter in word])\n",
    "\n",
    "# Numbers, to words with that gmatria\n",
    "\n",
    "def getGmatrias():\n",
    "    \n",
    "    gmatriasToWords = {}\n",
    "\n",
    "    directory = \"./texts/\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(\"./texts\")\n",
    "    \n",
    "    pathToGmatriaFile = \"{}bamidbarGmatriaByNumber.tsv\".format(directory)\n",
    "\n",
    "    if os.path.exists(pathToGmatriaFile):\n",
    "        print (\"Gmatria file exists. Reading...\")\n",
    "        reader = csv.reader(open(pathToGmatriaFile), delimiter = \"\\t\")\n",
    "        header = next(reader)\n",
    "        for line in reader:\n",
    "            line = dict(zip(header, line))\n",
    "            word = int(line[\"Gmatria\"])\n",
    "            shifts = set(line[\"Word\"].split(\" | \"))\n",
    "            gmatriasToWords[word] = shifts\n",
    "        print (\"Finished reading.\")\n",
    "    ### To-do: fill this in later\n",
    "#     else:\n",
    "#         print (\"Gmatria file does not exist. Creating...\")\n",
    "#         with open(pathToGmatriaFile, \"w\") as out:\n",
    "\n",
    "#             out.write(\"Gmatria\\tWord\\n\")\n",
    "\n",
    "#             \n",
    "    return gmatriasToWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gmatria file exists. Reading...\n",
      "Finished reading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'הארדי', 'וטהר', 'וידר', 'וירד', 'ורוח', 'טהור', 'יבחר', 'צפים', 'רוחו'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmatriasToWords = getGmatrias()\n",
    "gmatriasToWords[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames...\n",
      "vayikra\n",
      "dvarim\n",
      "breishit\n",
      "shmot\n",
      "bamidbar\n"
     ]
    }
   ],
   "source": [
    "folder = \"texts/Torah\"\n",
    "filenames = getFileNames(folder)\n",
    "\n",
    "sfarimToWords = {}\n",
    "for filename in filenames:\n",
    "    \n",
    "    seferName = filename.replace(\".txt\", \"\").split(\"/\")[-1]\n",
    "    print(seferName)\n",
    "    wordsInSefer = makeWordDictionaryFromSefer(filename)\n",
    "    sfarimToWords[seferName] = wordsInSefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bamidbar has 3847 words.\n",
      "bamidbar and dvarim share 1201 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and shmot share 1412 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and vayikra share 973 words.\n",
      "bamidbar has 3847 words.\n",
      "bamidbar and breishit share 1323 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and shmot share 1302 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and vayikra share 858 words.\n",
      "dvarim has 4089 words.\n",
      "dvarim and breishit share 1313 words.\n",
      "shmot has 4171 words.\n",
      "shmot and vayikra share 1007 words.\n",
      "shmot has 4171 words.\n",
      "shmot and breishit share 1482 words.\n",
      "vayikra has 2710 words.\n",
      "vayikra and breishit share 860 words.\n"
     ]
    }
   ],
   "source": [
    "sfarim = set(sfarimToWords)\n",
    "combos = itertools.combinations(sfarim, 2)\n",
    "for sefer1, sefer2 in combos:\n",
    "\n",
    "    sefer1words = set(sfarimToWords[sefer1])\n",
    "    sefer2words = set(sfarimToWords[sefer2])\n",
    "\n",
    "#     print (\"{} has {} words.\".format(sefer1, len(sefer1words)))\n",
    "#     print (\"{} has {} words.\".format(sefer2, len(sefer2words)))\n",
    "    print (\"{} and {} share {} words.\".format(sefer1, sefer2, len(sefer1words.intersection(sefer2words))))\n",
    "#     print (\"{} has {} unique words.\".format(sefer1, len(sefer1words.difference(sefer2words))))\n",
    "#     print (\"{} has {} unique words.\".format(sefer2, len(sefer2words.difference(sefer1words))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic histogram/breakdowns:\n",
    "    \n",
    "#### Letters, pairs of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talmud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames...\n",
      "There are 37 masechtot.\n"
     ]
    }
   ],
   "source": [
    "## Load:\n",
    "folder = \"texts/Talmud\"\n",
    "filenames = getFileNames(folder)\n",
    "print (\"There are {} masechtot.\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualRabbis = set([\"Hillel\", \"Rabbi Akiva\", \"Rabbi Ami\", \"Rabbi Shimon\", \"Shammai\", \"The Sages\", \"Abba Shaul\", \"Rabbi Yannai\", \"Rabbi Yehuda ben Agra\", \"Rav Ashi\", \"Rabbi Elazar, son of Rabbi Tzadok\", \"Rabbi Shimon ben Lakish\", \"Reish Lakish\", \"Rav Naḥman bar Yitzḥak\", \"Beit Hillel\", \"Rabbi Ḥanina\", \"Rabbi Yehuda\", \"Rabbi Elazar ben Azarya\", \"Rav Dimi\", \"Rabbi Shimon ben Elazar\", \"Rav Yosef\", \"Rav Huna bar Taḥlifa\", \"Rabbi Yishmael, son of Rabbi Yosei\", \"Rav Pappa\", \"Rabbi Abbahu\", \"Rabbi Yirmeya\", \"Rav Aḥa, son of Rava\", \"Rabbi Yitzḥak, son of Rabbi Bisna\", \"Rabbi Ḥanina ben Gamliel\", \"Rav Huna, son of Rav Natan\", \"Rabbi Yoḥanan\", \"Rava\", \"Rav Naḥman\", \"Beit Shammai\", \"Rabbi Yosei\", \"Rav Sheshet\", \"Rav Ḥiyya, son of Rav Huna\", \"Rabbi Eliezer\", \"Rav Ḥinnana bar Shelamya\", \"Rav Aḥa, son of Rav Yosef\", \"Rav Adda\", \"Rabban Gamliel\", \"Rabbi Neḥemya\", \"Rav Ḥanilai\", \"Rabbi Yehuda ben Bava\", \"Rabbi Abba bar Memel\", \"Rav Ḥanan from Neharde’a\", \"Rabbi Eliezer, son of Rabbi Tzadok\", \"Rav Huna, son of Rav Yehoshua\", \"Rabbi Yosei ben Keifar\", \"Shmuel\", \"Levi\", \"Rabbi Eliezer ben Ya’akov\", \"Rav Shimi bar Ḥiyya\", \"Rav Pineḥas\", \"Rav Shemaya\", \"Rabbi Amram\", \"Rav Huna\", \"Rav Ḥanina bar Shelamya\", \"Abaye\", \"Rav Aḥa, son of Rav Yeiva\", \"Rabbi Meir\", \"Rabbi Zeira\", \"Rabbi Yehoshua ben Levi\", \"Rabbi Yoḥanan ben Nuri\", \"Ameimar\", \"Rabbi Ḥanina ben Antigonus\", \"Rav Yehuda of Diskarta\", \"Rabbi Yehoshua\", \"Rav Shmuel bar Yeiva\", \"Rabbi Natan bar Yosef\", \"Rav Kahana\", \"Rav Huna bar Ḥiyya\", \"Rabbi Elazar\", \"Rabbi Yehoshua ben Ḥananya\", \"Ze’eiri\", \"Ami of Vardina\", \"Rav Yitzḥak, son of Rav Yehuda\", \"Rabba bar bar Ḥana\", \"Rabba bar bar Ḥana\", \"Rabbi Yishmael\", \"Ḥasa\", \"Ḥizkiya\", \"Ravin\", \"Rav Giddel\", \"Ḥananya ben Ḥizkiya\", \"Aḥer\", \"Rav Ḥiyya bar Abba\", \"Yosei ben Yo’ezer\", \"Yosef ben Yoḥanan\", \"Yehoshua ben Peraḥya\", \"Nitai HaArbeli\", \"Yehuda ben Tabbai\", \"Shimon ben Shataḥ\", \"Shemaya\", \"Avtalyon\", \"Rabbi Ila\", \"Rabbi Asi\", \"Rabbi Ḥiyya bar Abba\", \"Rav Sheizevi\", \"Rav Pinḥas\", \"Rabbi Musya, grandson of Rabbi Masya\", \"Rav Ḥisda\", \"Rav Shimi from Neharde’a\", \"Rav Yitzḥak\", \"Rabbi Sheila\", \"Rabban Yoḥanan ben Zakkai\", \"Isi ben Akiva\", \"Isi ben Yehuda\", \"Yosef of Hutzal\", \"Rav Mordekhai\", \"Rabbi Yoshiya\", \"Rav Ḥisda\", \"Rav Zutra bar Toviya\", \"Mar Zutra\", \"Rav Mattana\", \"Rav Yehuda\", \"Rabbi Abba\", \"Hanani\", \"Rabba bar Lima\", \"Rav Aḥa bar Ya’akov\", \"Rabba bar Avuh\", \"Rabbi Natan\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts/Talmud/Niddah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Chagigah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Yoma/English/merged.txt\n",
      "texts/Talmud/SederMoed/Rosh Hashanah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Moed Katan/English/merged.txt\n",
      "texts/Talmud/SederMoed/Beitzah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Sukkah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Megillah/English/merged.txt\n",
      "texts/Talmud/SederMoed/Taanit/English/merged.txt\n",
      "texts/Talmud/SederMoed/Pesachim/English/merged.txt\n",
      "texts/Talmud/SederMoed/Shabbat/English/merged.txt\n",
      "texts/Talmud/SederMoed/Eruvin/English/merged.txt\n",
      "texts/Talmud/Berakhot/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Horayot/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Avodah Zarah/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Bava Batra/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Shevuot/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Sanhedrin/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Bava Metzia/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Bava Kamma/English/merged.txt\n",
      "texts/Talmud/SederNezikin/Makkot/English/merged.txt\n",
      "texts/Talmud/SederNashim/Sotah/English/merged.txt\n",
      "texts/Talmud/SederNashim/Kiddushin/English/merged.txt\n",
      "texts/Talmud/SederNashim/Nazir/English/merged.txt\n",
      "texts/Talmud/SederNashim/Yevamot/English/merged.txt\n",
      "texts/Talmud/SederNashim/Gittin/English/merged.txt\n",
      "texts/Talmud/SederNashim/Ketubot/English/merged.txt\n",
      "texts/Talmud/SederNashim/Nedarim/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Zevachim/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Meilah/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Bekhorot/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Temurah/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Keritot/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Menachot/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Arakhin/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Chullin/English/merged.txt\n",
      "texts/Talmud/SederKodashim/Tamid/English/merged.txt\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lineNumber = 0\n",
    "people = set()\n",
    "for filename in filenames:\n",
    "    \n",
    "    print (filename)\n",
    "    weirdDelimiter = \"@\"\n",
    "    reader = open(filename, \"r\")\n",
    "    for line in reader:\n",
    "        \n",
    "\n",
    "        ### 1. Get sections with rabbis in them:\n",
    "\n",
    "        ### Whitespace/daf numbers. Translations have \"<\" in them\n",
    "        \n",
    "#         if \"<\" in line:\n",
    "            \n",
    "#             splitLine = line.split(\"<b>\")\n",
    "#             for chunk in splitLine:\n",
    "#                 if (\"Rabbi\" in chunk or \"Rav\" in chunk) and \"Rabbis\" not in chunk:\n",
    "#                     print (chunk)\n",
    "            \n",
    "        ### 2. Try it by verb:\n",
    "        \n",
    "#         splitLineByPhrase = line.split(\"<b>\")\n",
    "#         for phrase in splitLineByPhrase:\n",
    "            \n",
    "#             gotTheseRabbisAlready = False\n",
    "            \n",
    "            \n",
    "#             ### Skip ones we've found? Manual way\n",
    "#             ### Problems: If \"y\" is in the list, then \"x ben y\" will get skipped and we won't find him.\n",
    "#             ### Same with \"Rav\" and \"Rabba\" in general.\n",
    "#             for rabbi in manualRabbis:\n",
    "#                 if rabbi in phrase:\n",
    "#                     gotTheseRabbisAlready = True\n",
    "        \n",
    "#             if gotTheseRabbisAlready:\n",
    "#                 continue\n",
    "                \n",
    "            \n",
    "#             splitLine = phrase.split(\" \")\n",
    "#             verbs = [\"says\", \"holds\", \"applied\", \"said\"]\n",
    "#             for verb in verbs:\n",
    "#                 try:\n",
    "#                     verbIndex = splitLine.index(verb)\n",
    "#                     # By default, just start from the beginning of the line\n",
    "#                     rabbiIndex = 0\n",
    "#                     # But if \"Rabbi\" is in the line, start from that word\n",
    "#                     for rabbiWord in [\"Rabbi\", \"Rav\"]:\n",
    "\n",
    "#                         if rabbiWord in splitLine:\n",
    "#                             rabbiIndex = splitLine.index(rabbiWord)\n",
    "# #                     if not rabbiIndex:\n",
    "# #                         print (\"Rabbi word not in line\")\n",
    "\n",
    "\n",
    "#                     chunkContainingRabbi = splitLine[rabbiIndex : verbIndex]\n",
    "                    \n",
    "#                     ### Problem: Some times the rabbi's name is in the previous phrase,\n",
    "#                     ### and the new phrase starts with \"said\". For now, skip.                    \n",
    "#                     if chunkContainingRabbi == [\"said\"]:\n",
    "#                         continue\n",
    "                    \n",
    "#                     ### Skip ones that are generic pronouns\n",
    "#                     pronouns = [\"he\", \"she\", \"i\", \"they\"]\n",
    "#                     precedingWordIsNotPronoun = True\n",
    "#                     for pronoun in pronouns:\n",
    "#                         if splitLine[verbIndex - 1].lower() in pronouns:\n",
    "#                             precedingWordIsPronoun = False\n",
    "#                     if precedingWordIsNotPronoun and len(chunkContainingRabbi) > 1:\n",
    "#                         print (\" \".join(chunkContainingRabbi))\n",
    "#                 # If the verb isn't in the line.\n",
    "#                 except ValueError as e:\n",
    "#                     continue\n",
    "\n",
    "\n",
    "        ### 3. NLP\n",
    "    \n",
    "        ### Skip the header info, and the whitespace/daf numbers\n",
    "        lineNumber += 1\n",
    "        if lineNumber < 21 or len(line) < 13:\n",
    "            continue\n",
    "#         print (\"\\rLine number {}.\\r\".format(lineNumber))\n",
    "        line = re.sub('<[^<]+?>', '', line) # ayy https://stackoverflow.com/a/4869782\n",
    "        doc = nlp(line)\n",
    "        \n",
    "        ### This won't separate multi-word names\n",
    "#         for token in doc:\n",
    "#             if token.pos_ == \"PROPN\":\n",
    "#                 properNouns.add(token)\n",
    "                \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                people.add(ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleToText = {name.text for name in people}\n",
    "with open(\"talmudPeople.txt\", \"w\") as out:\n",
    "    out.write(\"\\n\".join(peopleToText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (general)",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
